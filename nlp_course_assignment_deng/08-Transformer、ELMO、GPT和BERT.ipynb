{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**definition**\n",
    "\n",
    "Autoencoder are neural networks that aims to copy their inputs to their inputs. They work by compressing the input into a latent-space-representation,and then reconstructing the output from this representation.\n",
    "\n",
    "This kind of network is composed of two parts:\n",
    "\n",
    "1、Encoder: This is the part of the network that compresses the input into a latent-space representation. It can be represented by an encoding function $h=f(x)$.\n",
    "\n",
    "2、Decoder: This part aims to reconstruct the input from the latent space representation.It can be represented by an decoding function $r=g(h)$.\n",
    "\n",
    "The autoencoder as a whole can thus be described by the function $g(f(x))=r$ where you want r as close as the original input x.\n",
    "\n",
    "**why we use autoencoder**\n",
    "\n",
    "Today data denoising and dimenstionality reduction for data visualization are considered as two main practical applications of autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy search choose the most likely word at each step in the output sequence.This approach has the benefit that it is very fast,but the quality of the final output sequences may be far from optimal.\n",
    "\n",
    "Instead of choosing the most likely next step as the sequence,beam search expands all the next steps and keeps the k most likely(k is beam width,and is typically larger than 3)\n",
    "\n",
    "Therefore, the differences between them are summarized as follows:\n",
    "\n",
    "1、Gready search is just a special case of beam search when the value of k is 1. Beam search with larger beam widths would achieve better performance of a model as the multiple candidate sequences increase the likelihood of better matching a target sequence.\n",
    "\n",
    "2、The decoding speed of beam search is lower than gready search because of considering k candidates at each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is a process of focusing on a smaller part of a large input stimuli.This means that any system applying attention will need to determine where to focus on.\n",
    "\n",
    "In NMT,the encoder of seq2seq without attention processes the input sequence into a context vector of a fixed length. However,there is a crirical disadvantage of this fixed-length context vector that the system would forget the earlier parts of the sequences once it has processed the entire sequence.\n",
    "\n",
    "However,attention allows machine translator to look over all the information the original sentence holds,then generate the proper word according to current context rather than solely based on last hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inability to handle unknown or OOV words**\n",
    "\n",
    "Word2vec is probably the most popular word embeding model.The biggest problem with word2vec is the inability to handle unkown or out-of-vocabulary words. If the model hasn't encountered a word before, it will have no idea how to build a vector for it.\n",
    "\n",
    "**No shared representations at sub-word levels**\n",
    "\n",
    "For example,if a new word ends in 'less', we can guess that it's probably an adjective indicating a lack of something like careless, thus words which are morphologically similar are expected to share representations at sub-word levels. However, word2vec represents every word as independent vector.\n",
    "\n",
    "**Word's representation is not change**\n",
    "\n",
    "The word embeddings are not context-specific:they are learned based on word concurrency but not sequential context. So in two sentences,\"I am eating an apply\" and \"I have an Apple phone\" ,two \"apple\" words refer to very different things but they would still share the same word embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a fixed embedding for each word, ELMo looks at the entire sentence before assigning each word in it an embedding. \n",
    "\n",
    "ELMO uses a bi-directional LSTM trained on a specific task to be able to create those embeddings.To this 2-layer network, a residual connection is added between the first and second layers which help deep models train more successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer relys entirly on self-attention to compute representations of its input and output without using RNN or CNN.\n",
    "\n",
    "There are two main advantages:\n",
    "\n",
    "1、transformer can handling long-range dependencies with ease because self-attention can relate different positions of a sequence, while at every time step in the encoder,RNN can only take a word vector from the previous time step.\n",
    "\n",
    "2、transformer allow the model to do parallel computing in the sequential input,thus traning neural networks with transformer architecture takes far less time than RNN which can only compute step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization normalizes the input features across the batch dimension, while the layer normalization normalizes the inputs across the feature dimension.\n",
    "\n",
    "In layer normalization,the statistics (mean and std) are computed across each feature and are independent of other examples,thus each input has a different normalization operation.\n",
    "\n",
    "LN techniques make training easier and more stable,but I don't kown why we use LN rather than BN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the transformer architecture ditched the recurrence mechanism in favor of multi-head self-attention mechanism. \n",
    "\n",
    "Postion and order of words are essential parts of any language, as they define the grammar and thus the actual semantics of a sentence.\n",
    "\n",
    "As each word in a sentence simultaneously flows through the transformer's encoder/decoder stack, the model itself doesn't have any sense of position for each word. Consequently, there's still the need for a way to incorporate the order of the words into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n",
    "\n",
    "Self-attention allows the model to look at the other words in the input sequence to get a better understanding of a certain word in the sequence. \n",
    "\n",
    "Self-attention is computed not once but multiple times in the Transformer’s architecture, in parallel and independently. It is therefore referred to as Multi-head Attention. \n",
    "\n",
    "According to the paper: Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first stage, generative pre-training of a language model can absorb as much free text as possible. Then at the second stage, the model is fine-tuned on specific tasks with a small labeled dataset and a minimal set of new parameters to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective allows the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT uses pairs of sentences as its training data.\n",
    "\n",
    "Imagine we have a text dataset of 100,000 sentences and we want to pre-train a BERT language model using this dataset. So, there will be 50,000 training examples or pairs of sentences as the training data.\n",
    "\n",
    "* For 50% of the pairs, the second sentence would actually be the next sentence to the first sentence.\n",
    "\n",
    "* For the remaining 50% of the pairs, the second sentence would be a random sentence from the corpus.\n",
    "\n",
    "* The labels for the first case would be ‘IsNext’ and ‘NotNext’ for the second case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In BERT, a model is first pre-trained with data that requires no human labeling. Once it is done, the pre-trained model outputs a dense representation of the input. To solve other NLP tasks, like QA, we modify the model by simply adding a shallow DL layer connecting to the output of the original model. Then, we retrain the model with data and labels specific to the task.\n",
    "\n",
    "In short, there is a pre-training phase in which we create a dense representation of the input (the left diagram below). The second phase retunes the model with task-specific data, like MNLI or SQuAD, to solve the target NLP problem. That is what we call \"fine-tuning\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT use unidirectional transformer to predict the next word in a sentence ,thus it was pre-trained using traditional language modeling.\n",
    "\n",
    "In contrast, BERT use bidirectional transformer,which was pre-trained using masked language modeling, and is more of a fill-in-the-blanks exercise: guessing missing (“masked”) words given the words that came before and after. This bidirectional architecture enabled BERT to learn richer representations.\n",
    "\n",
    "As GPT2,there were no fundamental algorithmic breakthroughs. It still choose unidirectional transformer and the brilliant ability of generation was a feat of scaling up: GPT-2 has a whopping 1.5 billion parameters (10X more than the original GPT) and is trained on the text from 8 million websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
