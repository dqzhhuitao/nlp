{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08111108, 0.55260899, 0.55282767, 0.80154683, 0.09385932,\n",
       "       0.86309379, 0.11394969, 0.32266401, 0.99820976, 0.58250899,\n",
       "       0.93236685, 0.09011821, 0.51686701, 0.3815499 , 0.55895762,\n",
       "       0.0640334 , 0.43759165, 0.62726974, 0.81190512, 0.40191704])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.random(20)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assuming_func(x):\n",
    "    return 13.4 * x + 5 + random.randint(-5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f15641086a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = [assuming_func(x) for x in X]\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用LR来拟合函数\n",
    "\"\"\"\n",
    "y = np.array(y)\n",
    "LR = LinearRegression()\n",
    "reg = LR.fit(X.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4797565006047161"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "R2值，越接近1说明拟合越好\n",
    "\"\"\"\n",
    "reg.score(X.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7.78817082]), 7.595575260318629)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "参数的拟合值，k和b\n",
    "\"\"\"\n",
    "reg.coef_,reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f15640e8eb8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"拟合好的函数\"\"\"\n",
    "\n",
    "def func(x):\n",
    "    return reg.coef_ * x + reg.intercept_\n",
    "\n",
    "y_pred = np.array([func(x) for x in X])\n",
    "\n",
    "plt.scatter(X.reshape(-1,1),y)\n",
    "plt.plot(X.reshape(-1,1),y_pred,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用欧式距离作为测度，找出和预测点最近的3个点，\n",
    "并求均值作为预测值。\n",
    "这个和分类不一样，是预测问题，\n",
    "而且变量只有一个，用余弦距离度量不合适。\n",
    "\"\"\"\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def model(X,y):\n",
    "    return [(x,y)for x,y in zip(X,y)]\n",
    "\n",
    "def distance(x1,x2):\n",
    "    return pdist([[x1,0],[x2,0]])\n",
    "\n",
    "def predict(x,k=3):\n",
    "    most_similars = sorted(model(X,y),key=lambda xi: distance(xi[0],x))[:k]\n",
    "    return np.mean([pair[1] for pair in most_similars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [predict (x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f15640160b8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X.reshape(-1,1),y)\n",
    "plt.scatter(X.reshape(-1,1),y_pred,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.061904311817917"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in counter]\n",
    "    return -sum([p * np.log(p) for p in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5623351446188083"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset['family_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "以 family number 作为分裂特征进行分裂\n",
    "\"\"\"\n",
    "sub_split_11 = dataset[dataset['family_number']==1]['bought'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_split_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_split_12 = dataset[dataset['family_number']!=1]['bought'].tolist()\n",
    "sub_split_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F', 'M'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "以性别作为分裂特征进行分裂\n",
    "\"\"\"\n",
    "\n",
    "sub_split_21 = dataset[dataset['gender']=='M']['bought'].tolist()\n",
    "sub_split_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_split_22 = dataset[dataset['gender']!='M']['bought'].tolist()\n",
    "sub_split_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6730116670092565"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(sub_split_11) + entropy(sub_split_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.198849312913621"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "可以看到以family number 作为分裂特征，分裂后的熵更小，\n",
    "则数据的混乱程度更小，进一步计算得到的信息增益更大。\n",
    "两相比较选择family number作为最佳分裂特征。\n",
    "\"\"\"\n",
    "\n",
    "entropy(sub_split_21) + entropy(sub_split_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'income', 'family_number', 'bought']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_min_spliter(trainning_data:pd.DataFrame,target:str)->str:\n",
    "    x_fields = set(trainning_data.columns.tolist()) - {target}\n",
    "    \n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(trainning_data[f])\n",
    "        ic(values)\n",
    "        \n",
    "        for v in values:\n",
    "            ic(v)\n",
    "            sub_spliter_1 = trainning_data[trainning_data[f]==v][target].tolist()\n",
    "            sub_spliter_2 = trainning_data[trainning_data[f]!=v][target].tolist()\n",
    "            ic(sub_spliter_1)\n",
    "            ic(sub_spliter_2)\n",
    "            \n",
    "            entropy_v = entropy(sub_spliter_1) + entropy(sub_spliter_2)\n",
    "            ic(entropy_v)\n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f,v)\n",
    "    print('Spliter is: {}'.format(spliter))\n",
    "    print('The min entropy is: {}'.format(min_entropy))\n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| v: 'F'\n",
      "ic| sub_spliter_1: [1, 1, 1, 0]\n",
      "ic| sub_spliter_2: [0, 0, 1]\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| v: 'M'\n",
      "ic| sub_spliter_1: [0, 0, 1]\n",
      "ic| sub_spliter_2: [1, 1, 1, 0]\n",
      "ic| entropy_v: 1.198849312913621\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| v: 1\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| v: 2\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| v: '+10'\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_v: 0.6730116670092565\n",
      "ic| v: '-10'\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_v: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliter is: ('income', '-10')\n",
      "The min entropy is: 0.6730116670092565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('income', '-10')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_min_spliter(dataset,'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "可以看到应该选择收入为首先分割的特征，\n",
    "收入为-10的时候，样本都是同一类，\n",
    "所以对收入为10的样本再进行划分\n",
    "\"\"\"\n",
    "\n",
    "sub_dataset_1 = dataset[dataset['income']=='+10']\n",
    "sub_dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F', 'M'}\n",
      "ic| v: 'F'\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| v: 'M'\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| sub_spliter_2: [1, 1, 0]\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| v: 1\n",
      "ic| sub_spliter_1: [1, 0, 0, 0]\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| v: 2\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| sub_spliter_2: [1, 0, 0, 0]\n",
      "ic| entropy_v: 0.5623351446188083\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10'}\n",
      "ic| v: '+10'\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_v: 0.6730116670092565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliter is: ('family_number', 2)\n",
      "The min entropy is: 0.5623351446188083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('family_number', 2)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_min_spliter(sub_dataset_1,'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "可以看到第二步的最佳分割特征为gender，\n",
    "gender为M时，都划分为一类，\n",
    "所以再对gender为F的子集进行分割\n",
    "\"\"\"\n",
    "\n",
    "sub_dataset_2 = sub_dataset_1[sub_dataset_1['gender']=='F']\n",
    "sub_dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F'}\n",
      "ic| v: 'F'\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_v: 0.6365141682948128\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| v: 1\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| v: 2\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10'}\n",
      "ic| v: '+10'\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_v: 0.6365141682948128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliter is: ('income', '+10')\n",
      "The min entropy is: 0.6365141682948128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('income', '+10')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_min_spliter(sub_dataset_2,'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "3      F    +10              1       0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_dataset_3 = sub_dataset_2[sub_dataset_2['family_number']==1]\n",
    "sub_dataset_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'gender'\n",
      "ic| values: {'F'}\n",
      "ic| v: 'F'\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| v: 1\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| f: 'income'\n",
      "ic| values: {'+10'}\n",
      "ic| v: '+10'\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_v: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliter is: ('gender', 'F')\n",
      "The min entropy is: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gender', 'F')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_min_spliter(sub_dataset_3,'bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三个特征划分完毕后，就不用再进行划分了**\n",
    "[![KhS8SS.md.jpg](https://s2.ax1x.com/2019/10/30/KhS8SS.md.jpg)](https://imgchr.com/i/KhS8SS=100*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [random.randint(0,100) for _ in range(100)]\n",
    "Y = [random.randint(0,100) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f155c99b940>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_date = [(x,y) for x,y in zip(X,Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "    n_clusters=6, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = KMeans(n_clusters=6,max_iter=500)\n",
    "cluster.fit(trainning_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.21052632, 10.78947368],\n",
       "       [70.47619048, 52.47619048],\n",
       "       [22.05882353, 84.52941176],\n",
       "       [77.        , 85.16666667],\n",
       "       [63.76190476, 18.23809524],\n",
       "       [18.1       , 44.5       ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 2, 0, 0, 0, 0, 3, 2, 3, 5, 4, 1, 0, 2, 0, 1, 0, 5, 2,\n",
       "       3, 0, 4, 1, 2, 4, 2, 3, 0, 1, 2, 2, 4, 2, 2, 3, 1, 1, 4, 1, 3, 1,\n",
       "       0, 2, 4, 3, 0, 4, 4, 5, 1, 4, 1, 3, 1, 1, 0, 0, 4, 2, 4, 4, 0, 0,\n",
       "       3, 5, 1, 4, 1, 1, 3, 3, 5, 3, 1, 1, 2, 4, 4, 4, 4, 0, 4, 5, 4, 5,\n",
       "       2, 4, 1, 2, 5, 4, 5, 0, 0, 1, 1, 5], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "centers = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label,location in zip(cluster.labels_,trainning_date):\n",
    "    centers[label].append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = ['red', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "\n",
    "for i,c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location,c=color[i])\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center,s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:**\n",
    "\n",
    "**Model can be regarded as a decision method to abstract,simplify and solve a problem.** In general, When we model a certain problem, we would observe the available data or other information, \n",
    "and choose some revelant factors (what we call 'features') as inputs,and then choose the target as output. Once we successfully build a model,we can make decisions quickly when we encounter a similar problem.\n",
    "\n",
    "**Answer 2:**\n",
    "\n",
    "**Because the mission of scientists is to prove that the previous works are wrong and to present their own theories.** There may be some problems in ML models which we have learned,but anyway the existing ML models can help us finish lots of difficult tastks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Overfitting**\n",
    "\n",
    "Overfitting is the case where the overall cost is really samll,but the generalization of the model is unreliable. This is due to the model learning too much from the dataset.\n",
    "\n",
    "**The possible reasons：**\n",
    "\n",
    "* the model is too complicated.\n",
    "* the size of dataset is too small.\n",
    "* the parameters of the model is too large.\n",
    "* the distribution of the trainning date is not correct,which means the model only capture a small part of the information in the real dataset.\n",
    "\n",
    "**2 Underfitting**\n",
    "\n",
    "Underfitting is the case where the model has not learned enough from the trainning data,resulting in low generalization and unreliable predictious.\n",
    "\n",
    "**The possible reasons:**\n",
    "\n",
    "* the model is too simple to capture the underlying trend of the data.\n",
    "\n",
    "* there are too few features used in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 The definition**\n",
    "\n",
    "First of all,we need to know what is confusion matrix. Let's assume that we have fed the trainning data to a model and received a classification. The predicted vs actual classification can be charted in a table called confusion matrix.\n",
    "\n",
    "[![KgQwrV.md.png](https://s2.ax1x.com/2019/10/28/KgQwrV.md.png)](https://imgchr.com/i/KgQwrV=100*50)\n",
    "\n",
    "* TN / True Negative: case was negative and predicted negative.\n",
    "* TP / True Positive: case was positive and predicted positive.\n",
    "* FN / False Negative: case was positive but predicted negative.\n",
    "* FP / False Positive: case was negative but predicted positive.\n",
    "\n",
    "Then,precision is defined as the number of TP over the number of TP plus the number of FP.$$ Precision=\\frac {TP}{TP+FP}$$\n",
    "\n",
    "Recall is defined as the number of TP over the number of TP plus the number of FN.$$ Pecall=\\frac {TP}{TP+FN}$$\n",
    "\n",
    "Fscore is defined as the harmonic mean of precision and recall.$$ F=(1+\\beta^2)×\\frac{P×R}{\\beta^2P+R}$$ If we consider that precision and recall are equal important($\\beta=1$) ,then we can get $$ F1=2×\\frac{P×R}{P+R}$$ If we consider recall to be more important($\\beta>1$), then we can get F2 score.\n",
    "\n",
    "AUC-ROC curve is a performance measurement for classification problem. ROC is a probability curve and AUC represents degree of separability. Higher the AUC, better the model is capable of distingushing between classes. AUC stands for area under the ROC curve.\n",
    "\n",
    "**2 Matters needing attention in use**\n",
    "\n",
    "Precision helps when the costs of false positives are high.When false positives are too high, those who monitor the results will learn to ignore them after being bombarded with false alarms.\n",
    "\n",
    "Recall helps when the cost of false negatives is high. When false negatives are frequent, you may get heart by the thing you want to avoid.\n",
    "\n",
    "Fscore is an overall measure of a model’s accuracy that combines precision and recall, in that weird way that addition and multiplication just mix two ingredients to make a separate dish altogether. IF we consider precision and recall to be equal important,then we can choose F1 score,while if recall is more important, we can choose F2 score.\n",
    "\n",
    "If we want to measure how well the positive classes are separated from the negative classes,we can use AUC-ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data,identify patterns and make decisions with minimal human intervention.\n",
    "\n",
    "**There are also some cases that ML solutions are not good choices**. \n",
    "\n",
    "For example,you don't need ML if you can determine a target value by using simple rules,computations,or predetermined steps that can be programmed without needing any data-driven learning. \n",
    "\n",
    "**We can using machine learning for the following situations**:\n",
    "\n",
    "* you cannot code the rules: many human tasks(such as recognizing whether an email is spam or not spam) cannot be adequately solved using a simple(deterministic),rule-based solution. A large number of factors could influence the answer. When rules depend on too many factors and many of the rules overlap or need to be tuned very finely, it soon becomes difficult for a human to accurately code the rules.\n",
    "\n",
    "* you cannot scale: you might be able to manually recognize a few hundred emails and decide whether they are spam or not. However, this task becomes tedious for millions of emails.ML solutions are effective at handling large-scale problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence above means the evaluation criterion of ML model is very important and need to be choosed carefully.\n",
    "\n",
    "There are several evaluation methods，but which is appropriate depends on the specific task.\n",
    "\n",
    "For example，if we want to build a model to predict the probability of having cancer，we'd better pay more attention to recall and F2 score. Because  false negatives can make us ignore the risk and miss the opportunity of timely treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用于处理字典类型的输入数据，\n",
    "得到纯数据（包括label)和特征标签\n",
    "\"\"\"\n",
    "\n",
    "def create_dataset(dataset:dict): \n",
    "    \"\"\"\n",
    "    :dataset: 字典类型的数据集，包含目标变量\n",
    "    \"\"\"\n",
    "    train_data = [v for v in dataset.values()]\n",
    "    train_data = list(map(list,zip(*train_data)))\n",
    "    labels = [k for i,k in enumerate(dataset.keys()) if i < len(dataset)-1]\n",
    "    return train_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "        'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "        'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "        'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "        'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "    }\n",
    "\n",
    "train_data,labels = create_dataset(mock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['F', '+10', 1, 1],\n",
       " ['F', '-10', 1, 1],\n",
       " ['F', '+10', 2, 1],\n",
       " ['F', '+10', 1, 0],\n",
       " ['M', '+10', 1, 0],\n",
       " ['M', '+10', 1, 0],\n",
       " ['M', '-10', 2, 1]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'income', 'family_number']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "用于计算数据集的熵\n",
    "\"\"\"\n",
    "\n",
    "def calc_entropy(dataset):  \n",
    "    \"\"\"\n",
    "    :dataset: 字典类型的数据集，包含目标变量\n",
    "    \"\"\"\n",
    "    size_dataset =len(dataset)  \n",
    "    labels = [data[-1] for data in dataset]\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    probs = [label_counts[c] / size_dataset for c in set(labels)]\n",
    "    entropy = -sum(p * log(p,2) for p in probs)\n",
    "    return round(entropy,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_entropy(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "根据某个特征和值对数据集进行分割\n",
    "得到样本子集合\n",
    "\"\"\"\n",
    "\n",
    "def split_dataset(dataset,index,value): \n",
    "    \"\"\"\n",
    "    :index: 特征对应的索引，用于取特征\n",
    "    :value: 特征的分割点\n",
    "    \"\"\"\n",
    "    sub_dataset=[]\n",
    "    for feat_vec in dataset:\n",
    "        if feat_vec[index] == value:\n",
    "            feat_vec_split =feat_vec[:index] + feat_vec[index+1:]\n",
    "            sub_dataset.append(feat_vec_split)\n",
    "    return sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['F', 1, 1], ['F', 2, 1], ['F', 1, 0], ['M', 1, 0], ['M', 1, 0]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset(train_data,1,'+10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "对所有的特征计算信息增益，信息增益最大的特征，\n",
    "作为最佳的特征，用于分裂\n",
    "\"\"\"\n",
    "\n",
    "def choose_best_feat(dataset): \n",
    "    \"\"\"\n",
    "    :best_feat: 返回值不是特征的名称，而是特征的索引\n",
    "    \"\"\"\n",
    "    num_feat = len(dataset[0])-1\n",
    "    base_entropy = calc_entropy(dataset)  \n",
    "    \n",
    "    best_info_gain = 0\n",
    "    best_feat = -1\n",
    "    for i in range(num_feat):\n",
    "        feat_list = [example[i] for example in dataset]\n",
    "        values = set(feat_list)\n",
    "        new_entropy = 0\n",
    "        for value in values:\n",
    "            sub_dataset = split_dataset(dataset,i,value)\n",
    "            prob = len(sub_dataset) / float(len(dataset))\n",
    "            new_entropy += prob * calc_entropy(sub_dataset)  \n",
    "        info_gain = base_entropy - new_entropy  \n",
    "        if (info_gain > best_info_gain):   \n",
    "            best_info_gain = info_gain\n",
    "            best_feat = i\n",
    "            \n",
    "    return best_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_best_feat(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'income'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[choose_best_feat(train_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "当叶节点的样本类型大于等于2时，取样本数最多的类型，\n",
    "作为该叶节点的分类。\n",
    "如[1,1,1,0,0]为叶节点的样本集，\n",
    "那么类型为1\n",
    "\"\"\"\n",
    "\n",
    "def major_count(class_list): \n",
    "    \"\"\"\n",
    "    :class_list: 叶节点的目标变量集合\n",
    "    \"\"\"\n",
    "    class_count = Counter(class_list)\n",
    "    class_count_sorted = sorted(class_count.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return class_count_sorted[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_count([1,1,1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "综合利用上面的函数和数据集，训练决策树模型。\n",
    "决策树的决策规则存为字典嵌套的格式。\n",
    "运用递归来创建决策树，递归停止条件有两个：\n",
    "1是子集合都属于同一种类型，2是已经对所有的特征都进行了分割（此时取样本最多的类型）\n",
    "\"\"\"\n",
    "\n",
    "def create_tree(dataset,labels):\n",
    "\n",
    "    class_list = [example[-1] for example in dataset]  \n",
    "    if class_list.count(class_list[0]) == len(class_list):\n",
    "        return class_list[0]\n",
    "    if len(dataset[0]) == 1:\n",
    "        return major_count(class_list)\n",
    "    \n",
    "    best_feat = choose_best_feat(dataset) \n",
    "    best_feat_label = labels[best_feat]\n",
    "    myTree = {best_feat_label:{}} \n",
    "    del(labels[best_feat])\n",
    "    \n",
    "    feat_values = [example[best_feat] for example in dataset]\n",
    "    unique_vals = set(feat_values)\n",
    "    for value in unique_vals:\n",
    "        sub_labels = labels[:]\n",
    "        myTree[best_feat_label][value] = create_tree(split_dataset\\\n",
    "                            (dataset,best_feat,value),sub_labels)\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'income': {'+10': {'gender': {'F': {'family_number': {1: 1, 2: 1}}, 'M': 0}},\n",
       "  '-10': 1}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset,labels = create_dataset(mock_data)\n",
    "my_tree = create_tree(dataset, labels)\n",
    "\n",
    "my_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "对新样本进行预测\n",
    "\"\"\"\n",
    "\n",
    "def predict(X:dict,model):\n",
    "    \"\"\"\n",
    "    :X: 待预测的数据，字典格式\n",
    "    :model: 以字典嵌套形式保存的决策树\n",
    "    \"\"\"\n",
    "    split_feature = list(model.keys())[0]\n",
    "    value = X[split_feature]\n",
    "    result = model[split_feature][value]\n",
    "    if type(result).__name__ != 'dict':\n",
    "        return result\n",
    "    else:\n",
    "        return predict(X, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 'F', 'income': '+10', 'family_number': 2}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ['F','+10',2]\n",
    "labels = ['gender','income','family_number']\n",
    "X_test = dict(zip(labels,example))  \n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_test,my_tree)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9c3d65ef98>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_rm = x[:,5]\n",
    "\n",
    "plt.scatter(X_rm,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "假设目标函数为一元线性回归函数，\n",
    "用梯度下降来求参数\n",
    "\"\"\"\n",
    "\n",
    "def price(rm,k,b):\n",
    "    return k * rm + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数为绝对值损失函数\n",
    "\n",
    "$$ loss = \\frac{1}{n} \\sum{|y_i - \\hat{y_i}|}$$\n",
    "\n",
    "$$ loss = \\frac{1}{n} \\sum{|y_i - (kx_i + b_i)|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义损失函数为绝对值损失函数\n",
    "\"\"\"\n",
    "\n",
    "def loss(y,y_hat):\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat))) / len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算k和b的偏导数，是分段函数\n",
    "\n",
    "$$ \\frac{\\partial{loss}}{\\partial{k}} = \\begin{cases}\n",
    "-\\frac{1}{n}\\sum(x_i) & y_i > \\hat{y_i} \\\\\n",
    "0 & y_i = \\hat{y_i} \\\\\n",
    "\\frac{1}{n}\\sum(x_i) & y_i < \\hat{y_i}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$ \\frac{\\partial{loss}}{\\partial{b}} = \\begin{cases}\n",
    "-\\frac{1}{n} & y_i > \\hat{y_i} \\\\\n",
    "0 & y_i = \\hat{y_i} \\\\\n",
    "\\frac{1}{n} & y_i < \\hat{y_i}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "计算损失关于k和b的偏导数\n",
    "\"\"\"\n",
    "def get_sign(y_i,y_hat_i):\n",
    "    if (y_i - y_hat_i) == 0:\n",
    "        sign = 0\n",
    "    else: \n",
    "        sign = (y_i - y_hat_i) / abs(y_i - y_hat_i)\n",
    "    return sign\n",
    "        \n",
    "def partial_derivative_k(x,y,y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i,y_i,y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        sign = get_sign(y_i,y_hat_i)\n",
    "        gradient += sign * x_i\n",
    "    return -1/n * gradient\n",
    "\n",
    "def partial_derivative_b(y,y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i,y_hat_i in zip(list(y),list(y_hat)):\n",
    "        sign = get_sign(y_i,y_hat_i)\n",
    "        gradient += sign\n",
    "    return -1/n * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 401.8549128692984, parameters k is 75.98898835470402 and b is -53.17529008063777\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 1, the loss is 397.8052499310298, parameters k is 75.36052491596884 and b is -53.275290080637774\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 2, the loss is 393.7555869927612, parameters k is 74.73206147723366 and b is -53.375290080637775\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 3, the loss is 389.7059240544931, parameters k is 74.10359803849848 and b is -53.47529008063778\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 4, the loss is 385.6562611162246, parameters k is 73.4751345997633 and b is -53.57529008063778\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 5, the loss is 381.6065981779561, parameters k is 72.84667116102811 and b is -53.67529008063778\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 6, the loss is 377.5569352396871, parameters k is 72.21820772229293 and b is -53.77529008063778\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 7, the loss is 373.50727230141894, parameters k is 71.58974428355775 and b is -53.87529008063778\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 8, the loss is 369.45760936315054, parameters k is 70.96128084482257 and b is -53.975290080637784\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 9, the loss is 365.407946424882, parameters k is 70.33281740608739 and b is -54.075290080637785\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 10, the loss is 361.3582834866134, parameters k is 69.7043539673522 and b is -54.17529008063779\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 11, the loss is 357.3086205483453, parameters k is 69.07589052861702 and b is -54.27529008063779\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 12, the loss is 353.2589576100766, parameters k is 68.44742708988184 and b is -54.37529008063779\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 13, the loss is 349.2092946718075, parameters k is 67.81896365114666 and b is -54.47529008063779\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 14, the loss is 345.15963173353975, parameters k is 67.19050021241148 and b is -54.57529008063779\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 15, the loss is 341.1099687952714, parameters k is 66.5620367736763 and b is -54.675290080637794\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 16, the loss is 337.0603058570026, parameters k is 65.93357333494112 and b is -54.775290080637795\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 17, the loss is 333.0106429187344, parameters k is 65.30510989620593 and b is -54.8752900806378\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 18, the loss is 328.96097998046565, parameters k is 64.67664645747075 and b is -54.9752900806378\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 19, the loss is 324.9113170421976, parameters k is 64.04818301873557 and b is -55.0752900806378\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 20, the loss is 320.86165410392863, parameters k is 63.41971958000039 and b is -55.1752900806378\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 21, the loss is 316.81199116566023, parameters k is 62.79125614126521 and b is -55.2752900806378\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 22, the loss is 312.7623282273919, parameters k is 62.16279270253003 and b is -55.375290080637804\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 23, the loss is 308.71266528912327, parameters k is 61.534329263794845 and b is -55.475290080637805\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 24, the loss is 304.66300235085544, parameters k is 60.905865825059664 and b is -55.57529008063781\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 25, the loss is 300.6133394125862, parameters k is 60.27740238632448 and b is -55.67529008063781\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 26, the loss is 296.5636764743181, parameters k is 59.6489389475893 and b is -55.77529008063781\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 27, the loss is 292.51401353604956, parameters k is 59.02047550885412 and b is -55.87529008063781\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 28, the loss is 288.4643505977809, parameters k is 58.39201207011894 and b is -55.97529008063781\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 29, the loss is 284.41468765951277, parameters k is 57.763548631383756 and b is -56.075290080637814\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 30, the loss is 280.36502472124437, parameters k is 57.135085192648575 and b is -56.175290080637815\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 31, the loss is 276.31536178297586, parameters k is 56.50662175391339 and b is -56.27529008063782\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 32, the loss is 272.26569884470706, parameters k is 55.87815831517821 and b is -56.37529008063782\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 33, the loss is 268.2160359064388, parameters k is 55.24969487644303 and b is -56.47529008063782\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 34, the loss is 264.16637296817044, parameters k is 54.62123143770785 and b is -56.57529008063782\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 35, the loss is 260.1167100299019, parameters k is 53.99276799897267 and b is -56.67529008063782\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 36, the loss is 256.06704709163347, parameters k is 53.364304560237485 and b is -56.775290080637824\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 37, the loss is 252.0173841533649, parameters k is 52.735841121502304 and b is -56.875290080637825\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 38, the loss is 247.96772121509645, parameters k is 52.10737768276712 and b is -56.975290080637826\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 39, the loss is 243.91805827682788, parameters k is 51.47891424403194 and b is -57.07529008063783\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 40, the loss is 239.86839533855957, parameters k is 50.85045080529676 and b is -57.17529008063783\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 41, the loss is 235.81873240029128, parameters k is 50.22198736656158 and b is -57.27529008063783\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 42, the loss is 231.76906946202263, parameters k is 49.593523927826396 and b is -57.37529008063783\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 43, the loss is 227.71940652375412, parameters k is 48.965060489091215 and b is -57.47529008063783\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 44, the loss is 223.66974358548558, parameters k is 48.33659705035603 and b is -57.575290080637835\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 45, the loss is 219.6200806472172, parameters k is 47.70813361162085 and b is -57.675290080637836\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 46, the loss is 215.57041770894858, parameters k is 47.07967017288567 and b is -57.77529008063784\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 47, the loss is 211.52075477068018, parameters k is 46.45120673415049 and b is -57.87529008063784\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 48, the loss is 207.47109183241164, parameters k is 45.82274329541531 and b is -57.97529008063784\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 49, the loss is 203.42142889414313, parameters k is 45.194279856680126 and b is -58.07529008063784\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 50, the loss is 199.3717659558748, parameters k is 44.565816417944944 and b is -58.17529008063784\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 51, the loss is 195.32210301760617, parameters k is 43.93735297920976 and b is -58.275290080637845\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 52, the loss is 191.2724400793378, parameters k is 43.30888954047458 and b is -58.375290080637846\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 53, the loss is 187.2227771410694, parameters k is 42.6804261017394 and b is -58.47529008063785\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 54, the loss is 183.17311420280086, parameters k is 42.05196266300422 and b is -58.57529008063785\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 55, the loss is 179.1234512645324, parameters k is 41.42349922426904 and b is -58.67529008063785\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 56, the loss is 175.07378832626387, parameters k is 40.795035785533855 and b is -58.77529008063785\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 57, the loss is 171.02412538799547, parameters k is 40.166572346798674 and b is -58.87529008063785\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 58, the loss is 166.97446244972676, parameters k is 39.53810890806349 and b is -58.975290080637855\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 59, the loss is 162.92479951145867, parameters k is 38.90964546932831 and b is -59.075290080637856\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 60, the loss is 158.87513657319016, parameters k is 38.28118203059313 and b is -59.17529008063786\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 61, the loss is 154.82547363492168, parameters k is 37.65271859185795 and b is -59.27529008063786\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 62, the loss is 150.77581069665308, parameters k is 37.024255153122766 and b is -59.37529008063786\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 63, the loss is 146.72614775838449, parameters k is 36.395791714387585 and b is -59.47529008063786\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 64, the loss is 142.67648482011614, parameters k is 35.7673282756524 and b is -59.57529008063786\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 65, the loss is 138.62682188184766, parameters k is 35.13886483691722 and b is -59.675290080637865\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 66, the loss is 134.5771589435793, parameters k is 34.51040139818204 and b is -59.775290080637866\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 67, the loss is 130.5274960053108, parameters k is 33.88193795944686 and b is -59.87529008063787\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 68, the loss is 126.4778330670422, parameters k is 33.25347452071168 and b is -59.97529008063787\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 69, the loss is 122.42817012877373, parameters k is 32.625011081976496 and b is -60.07529008063787\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 70, the loss is 118.37850719050533, parameters k is 31.996547643241318 and b is -60.17529008063787\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 71, the loss is 114.328844252237, parameters k is 31.36808420450614 and b is -60.27529008063787\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 72, the loss is 110.27918131396835, parameters k is 30.73962076577096 and b is -60.375290080637875\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 73, the loss is 106.22951837569997, parameters k is 30.111157327035784 and b is -60.475290080637876\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 74, the loss is 102.17985543743157, parameters k is 29.482693888300606 and b is -60.57529008063788\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 75, the loss is 98.13019249916313, parameters k is 28.854230449565428 and b is -60.67529008063788\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 76, the loss is 94.08052956089468, parameters k is 28.22576701083025 and b is -60.77529008063788\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 77, the loss is 90.03086662262626, parameters k is 27.597303572095072 and b is -60.87529008063788\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 78, the loss is 85.98120368435775, parameters k is 26.968840133359894 and b is -60.97529008063788\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 79, the loss is 81.93154074608934, parameters k is 26.340376694624716 and b is -61.075290080637885\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 80, the loss is 77.88187780782087, parameters k is 25.711913255889538 and b is -61.175290080637886\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 81, the loss is 73.83221486955233, parameters k is 25.08344981715436 and b is -61.27529008063789\n",
      "k_gradient is 6.284634387351787\n",
      "b_gradient is 0.9999999999999999\n",
      "Iteration 82, the loss is 69.78963131305109, parameters k is 24.454986378419182 and b is -61.37529008063789\n",
      "k_gradient is 6.270559288537557\n",
      "b_gradient is 0.9960474308300394\n",
      "Iteration 83, the loss is 65.75842888549835, parameters k is 23.827930449565425 and b is -61.474894823720895\n",
      "k_gradient is 6.270559288537557\n",
      "b_gradient is 0.9960474308300394\n",
      "Iteration 84, the loss is 61.72722645794551, parameters k is 23.20087452071167 and b is -61.5744995668039\n",
      "k_gradient is 6.270559288537557\n",
      "b_gradient is 0.9960474308300394\n",
      "Iteration 85, the loss is 57.696024030392785, parameters k is 22.57381859185791 and b is -61.67410430988691\n",
      "k_gradient is 6.270559288537557\n",
      "b_gradient is 0.9960474308300394\n",
      "Iteration 86, the loss is 53.67585584593727, parameters k is 21.946762663004154 and b is -61.77370905296991\n",
      "k_gradient is 6.235646245059297\n",
      "b_gradient is 0.9881422924901185\n",
      "Iteration 87, the loss is 49.68988491756434, parameters k is 21.323198038498223 and b is -61.87252328221893\n",
      "k_gradient is 6.235646245059297\n",
      "b_gradient is 0.9881422924901185\n",
      "Iteration 88, the loss is 45.70391398919134, parameters k is 20.69963341399229 and b is -61.971337511467944\n",
      "k_gradient is 6.235646245059297\n",
      "b_gradient is 0.9881422924901185\n",
      "Iteration 89, the loss is 41.71794306081838, parameters k is 20.07606878948636 and b is -62.07015174071696\n",
      "k_gradient is 6.235646245059297\n",
      "b_gradient is 0.9881422924901185\n",
      "Iteration 90, the loss is 37.73197213244538, parameters k is 19.45250416498043 and b is -62.168965969965974\n",
      "k_gradient is 6.235646245059297\n",
      "b_gradient is 0.9881422924901185\n",
      "Iteration 91, the loss is 33.75251409102469, parameters k is 18.828939540474497 and b is -62.26778019921499\n",
      "k_gradient is 6.21242490118578\n",
      "b_gradient is 0.9841897233201581\n",
      "Iteration 92, the loss is 29.79948080603974, parameters k is 18.207697050355918 and b is -62.366199171547\n",
      "k_gradient is 6.1960691699604835\n",
      "b_gradient is 0.9802371541501975\n",
      "Iteration 93, the loss is 25.88292850639211, parameters k is 17.58809013335987 and b is -62.46422288696202\n",
      "k_gradient is 6.1551442687747135\n",
      "b_gradient is 0.9723320158102766\n",
      "Iteration 94, the loss is 22.008259757224877, parameters k is 16.9725757064824 and b is -62.561456088543046\n",
      "k_gradient is 6.117235177865621\n",
      "b_gradient is 0.9644268774703557\n",
      "Iteration 95, the loss is 18.186307118448813, parameters k is 16.360852188695837 and b is -62.65789877629008\n",
      "k_gradient is 6.090820158102775\n",
      "b_gradient is 0.9604743083003952\n",
      "Iteration 96, the loss is 14.432032050170983, parameters k is 15.751770172885559 and b is -62.75394620712012\n",
      "k_gradient is 5.936175889328071\n",
      "b_gradient is 0.9328063241106719\n",
      "Iteration 97, the loss is 10.919905632919196, parameters k is 15.158152583952752 and b is -62.84722683953119\n",
      "k_gradient is 5.566847826086964\n",
      "b_gradient is 0.8616600790513833\n",
      "Iteration 98, the loss is 7.822209363983813, parameters k is 14.601467801344056 and b is -62.93339284743633\n",
      "k_gradient is 5.213942687747042\n",
      "b_gradient is 0.8023715415019762\n",
      "Iteration 99, the loss is 5.558312277790548, parameters k is 14.080073532569351 and b is -63.01363000158653\n",
      "k_gradient is 2.774519762845848\n",
      "b_gradient is 0.40711462450592883\n",
      "Iteration 100, the loss is 4.97512839319019, parameters k is 13.802621556284766 and b is -63.05434146403712\n",
      "k_gradient is 1.0710731225296446\n",
      "b_gradient is 0.13438735177865613\n",
      "Iteration 101, the loss is 4.890804102486126, parameters k is 13.695514244031802 and b is -63.06778019921499\n",
      "k_gradient is 0.3797094861660079\n",
      "b_gradient is 0.023715415019762844\n",
      "Iteration 102, the loss is 4.881994278566619, parameters k is 13.657543295415202 and b is -63.070151740716966\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 103, the loss is 4.881756029170051, parameters k is 13.653824915968562 and b is -63.066989685380996\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 104, the loss is 4.88161195974665, parameters k is 13.650106536521921 and b is -63.063827630045026\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 105, the loss is 4.88145026419178, parameters k is 13.650943097786744 and b is -63.059875060875065\n",
      "k_gradient is 0.015365612648220516\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 106, the loss is 4.881320982417672, parameters k is 13.649406536521921 and b is -63.0563177486221\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 107, the loss is 4.881172358330893, parameters k is 13.650243097786744 and b is -63.05236517945214\n",
      "k_gradient is 0.015365612648220516\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 108, the loss is 4.881030005088707, parameters k is 13.648706536521921 and b is -63.04880786719917\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 109, the loss is 4.88089691334649, parameters k is 13.649543097786744 and b is -63.04485529802921\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 110, the loss is 4.88077290280856, parameters k is 13.645824718340103 and b is -63.04169324269324\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 111, the loss is 4.88060967643063, parameters k is 13.646661279604926 and b is -63.03774067352328\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 112, the loss is 4.880470912145492, parameters k is 13.647497840869748 and b is -63.03378810435332\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 113, the loss is 4.880352574150481, parameters k is 13.643779461423108 and b is -63.03062604901735\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 114, the loss is 4.880189347772552, parameters k is 13.64461602268793 and b is -63.02667347984739\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 115, the loss is 4.880044910944487, parameters k is 13.645452583952753 and b is -63.02272091067743\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 116, the loss is 4.8799322454924, parameters k is 13.641734204506113 and b is -63.01955885534146\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 117, the loss is 4.87976901911447, parameters k is 13.642570765770936 and b is -63.0156062861715\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 118, the loss is 4.879619088683267, parameters k is 13.643407327035758 and b is -63.011653717001536\n",
      "k_gradient is 0.015365612648220516\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 119, the loss is 4.879478041785502, parameters k is 13.641870765770935 and b is -63.00809640474857\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 120, the loss is 4.879355414481097, parameters k is 13.642707327035758 and b is -63.00414383557861\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 121, the loss is 4.879220939505353, parameters k is 13.638988947589118 and b is -63.00098178024264\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 122, the loss is 4.879057713127419, parameters k is 13.63982550885394 and b is -62.99702921107268\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 123, the loss is 4.878929413280091, parameters k is 13.640662070118763 and b is -62.99307664190272\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 124, the loss is 4.878800610847275, parameters k is 13.636943690672123 and b is -62.98991458656675\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 125, the loss is 4.878637384469342, parameters k is 13.637780251936945 and b is -62.98596201739679\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 126, the loss is 4.878503412079084, parameters k is 13.638616813201768 and b is -62.982009448226826\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 127, the loss is 4.878380282189189, parameters k is 13.634898433755128 and b is -62.978847392890856\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 128, the loss is 4.878217055811264, parameters k is 13.63573499501995 and b is -62.974894823720895\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 129, the loss is 4.878077410878091, parameters k is 13.636571556284773 and b is -62.970942254550934\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 130, the loss is 4.8779599535311124, parameters k is 13.632853176838132 and b is -62.967780199214964\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 131, the loss is 4.877796727153183, parameters k is 13.633689738102955 and b is -62.963827630045\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 132, the loss is 4.877651409677084, parameters k is 13.634526299367778 and b is -62.95987506087504\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 133, the loss is 4.877545836798279, parameters k is 13.630807919921137 and b is -62.95671300553907\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 134, the loss is 4.8774014064002005, parameters k is 13.63418914521758 and b is -62.952365179452116\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 135, the loss is 4.877245612116235, parameters k is 13.630470765770939 and b is -62.949203124116146\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 136, the loss is 4.877082385738302, parameters k is 13.631307327035762 and b is -62.945250554946185\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 137, the loss is 4.876975405199199, parameters k is 13.632143888300584 and b is -62.941297985776224\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 138, the loss is 4.8768252834581585, parameters k is 13.628425508853944 and b is -62.938135930440254\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 139, the loss is 4.876662057080228, parameters k is 13.629262070118767 and b is -62.93418336127029\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 140, the loss is 4.876549403998195, parameters k is 13.63009863138359 and b is -62.93023079210033\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 141, the loss is 4.876406664977128, parameters k is 13.626380251936949 and b is -62.92706873676436\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 142, the loss is 4.876299400721313, parameters k is 13.62976147723339 and b is -62.922720910677405\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 143, the loss is 4.876110942043275, parameters k is 13.62604309778675 and b is -62.919558855341435\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 144, the loss is 4.87596727542253, parameters k is 13.626879659051573 and b is -62.915606286171474\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 145, the loss is 4.875879657600044, parameters k is 13.623161279604933 and b is -62.912444230835504\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 146, the loss is 4.875717272145646, parameters k is 13.626542504901375 and b is -62.90809640474855\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 147, the loss is 4.875564540957851, parameters k is 13.622824125454734 and b is -62.90493434941258\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 148, the loss is 4.8754672688687615, parameters k is 13.626205350751176 and b is -62.90058652332562\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 149, the loss is 4.875265814249531, parameters k is 13.622486971304536 and b is -62.89742446798965\n",
      "k_gradient is -0.008365612648221984\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 150, the loss is 4.875135143569978, parameters k is 13.623323532569358 and b is -62.89347189881969\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 151, the loss is 4.875037533580762, parameters k is 13.619605153122718 and b is -62.89030984348372\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 152, the loss is 4.874885140293092, parameters k is 13.62298637841916 and b is -62.885962017396764\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 153, the loss is 4.874722416938572, parameters k is 13.61926799897252 and b is -62.882799962060794\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 154, the loss is 4.874635137016208, parameters k is 13.622649224268962 and b is -62.87845213597384\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 155, the loss is 4.874425461881792, parameters k is 13.618930844822321 and b is -62.87529008063787\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 156, the loss is 4.874298120713432, parameters k is 13.617585587905324 and b is -62.8717327683849\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 157, the loss is 4.874222627800709, parameters k is 13.620966813201766 and b is -62.867384942297946\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 158, the loss is 4.874009134493845, parameters k is 13.617248433755126 and b is -62.864222886961976\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 159, the loss is 4.8738738244882915, parameters k is 13.61590317683813 and b is -62.86066557470901\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 160, the loss is 4.873810118585205, parameters k is 13.619284402134571 and b is -62.856317748622054\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 161, the loss is 4.873592807105906, parameters k is 13.615566022687931 and b is -62.853155693286084\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 162, the loss is 4.873453086547825, parameters k is 13.614220765770934 and b is -62.84959838103312\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 163, the loss is 4.8733289793333086, parameters k is 13.615420172885559 and b is -62.84564581186316\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 164, the loss is 4.873225368892592, parameters k is 13.611701793438918 and b is -62.84248375652719\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 165, the loss is 4.873078976056424, parameters k is 13.61508301873536 and b is -62.83813593044023\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 166, the loss is 4.8729102522504, parameters k is 13.61136463928872 and b is -62.83497387510426\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 167, the loss is 4.8728289727795415, parameters k is 13.614745864585162 and b is -62.830626049017305\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 168, the loss is 4.872617818137552, parameters k is 13.611027485138521 and b is -62.827463993681334\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 169, the loss is 4.872492039892273, parameters k is 13.609682228221525 and b is -62.82390668142837\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 170, the loss is 4.872348706150957, parameters k is 13.610881635336149 and b is -62.81995411225841\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 171, the loss is 4.872204064284584, parameters k is 13.609536378419152 and b is -62.81639680000544\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 172, the loss is 4.87207248584714, parameters k is 13.608191121502156 and b is -62.81283948775248\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 173, the loss is 4.871942437733681, parameters k is 13.60939052861678 and b is -62.808886918582516\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 174, the loss is 4.8718310319985605, parameters k is 13.60567214917014 and b is -62.805724863246546\n",
      "k_gradient is -0.03381225296442746\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 175, the loss is 4.871692434456806, parameters k is 13.609053374466582 and b is -62.80137703715959\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 176, the loss is 4.871528698718771, parameters k is 13.605334995019941 and b is -62.79821498182362\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 177, the loss is 4.871376290717578, parameters k is 13.606534402134566 and b is -62.79426241265366\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 178, the loss is 4.87123361479385, parameters k is 13.605189145217569 and b is -62.79070510040069\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 179, the loss is 4.87113091128818, parameters k is 13.606388552332193 and b is -62.78675253123073\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 180, the loss is 4.870982617072438, parameters k is 13.602670172885553 and b is -62.78359047589476\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 181, the loss is 4.870820202672147, parameters k is 13.603869580000177 and b is -62.7796379067248\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 182, the loss is 4.87068753314752, parameters k is 13.60252432308318 and b is -62.776080594471836\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 183, the loss is 4.870569388119573, parameters k is 13.603723730197805 and b is -62.772128025301875\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 184, the loss is 4.870436535426111, parameters k is 13.600005350751164 and b is -62.768965969965905\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 185, the loss is 4.870265921621405, parameters k is 13.601204757865789 and b is -62.765013400795944\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 186, the loss is 4.870170370889573, parameters k is 13.602404164980413 and b is -62.76106083162598\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 187, the loss is 4.870014923899989, parameters k is 13.598685785533773 and b is -62.75789877629001\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 188, the loss is 4.869852668447661, parameters k is 13.599885192648397 and b is -62.75394620712005\n",
      "k_gradient is 0.013452569169959752\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 189, the loss is 4.869719839975079, parameters k is 13.5985399357314 and b is -62.750388894867086\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 190, the loss is 4.869608847720953, parameters k is 13.599739342846025 and b is -62.746436325697125\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 191, the loss is 4.869468842253659, parameters k is 13.596020963399384 and b is -62.743274270361155\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 192, the loss is 4.869298228448955, parameters k is 13.597220370514009 and b is -62.739321701191194\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 193, the loss is 4.869209830490956, parameters k is 13.598419777628633 and b is -62.73536913202123\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 194, the loss is 4.869047230727548, parameters k is 13.594701398181993 and b is -62.73220707668526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 195, the loss is 4.86889119717767, parameters k is 13.595900805296617 and b is -62.7282545075153\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 196, the loss is 4.868796233006136, parameters k is 13.592182425849977 and b is -62.72509245217933\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 197, the loss is 4.868625619201432, parameters k is 13.593381832964601 and b is -62.72113988300937\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 198, the loss is 4.868492179947673, parameters k is 13.594581240079226 and b is -62.71718731383941\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 199, the loss is 4.868374621480023, parameters k is 13.590862860632585 and b is -62.71402525850344\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 200, the loss is 4.868204007675323, parameters k is 13.59206226774721 and b is -62.71007268933348\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 201, the loss is 4.868093162717674, parameters k is 13.593261674861834 and b is -62.70612012016352\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 202, the loss is 4.867953009953902, parameters k is 13.589543295415194 and b is -62.70295806482755\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 203, the loss is 4.867782396149203, parameters k is 13.590742702529818 and b is -62.69900549565759\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 204, the loss is 4.867694145487671, parameters k is 13.591942109644442 and b is -62.69505292648763\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 205, the loss is 4.867531398427791, parameters k is 13.588223730197802 and b is -62.69189087115166\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 206, the loss is 4.867375512174389, parameters k is 13.589423137312426 and b is -62.687938301981696\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 207, the loss is 4.867280400706379, parameters k is 13.585704757865786 and b is -62.684776246645725\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 208, the loss is 4.8671097869016755, parameters k is 13.58690416498041 and b is -62.680823677475765\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 209, the loss is 4.866976494944388, parameters k is 13.588103572095035 and b is -62.676871108305804\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 210, the loss is 4.866858789180264, parameters k is 13.584385192648394 and b is -62.67370905296983\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 211, the loss is 4.866688175375565, parameters k is 13.585584599763019 and b is -62.66975648379987\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 212, the loss is 4.866577477714389, parameters k is 13.586784006877643 and b is -62.66580391462991\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 213, the loss is 4.866437177654155, parameters k is 13.583065627431003 and b is -62.66264185929394\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 214, the loss is 4.8662714570403125, parameters k is 13.584265034545627 and b is -62.65868929012398\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 215, the loss is 4.866140036248261, parameters k is 13.583091319130608 and b is -62.655131977871015\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 216, the loss is 4.866022333109727, parameters k is 13.584290726245232 and b is -62.651179408701054\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 217, the loss is 4.865889038526835, parameters k is 13.580572346798592 and b is -62.648017353365084\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 218, the loss is 4.865721953679845, parameters k is 13.581771753913216 and b is -62.64406478419512\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 219, the loss is 4.865591897120941, parameters k is 13.580598038498197 and b is -62.64050747194216\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 220, the loss is 4.865467188505055, parameters k is 13.581797445612821 and b is -62.6365549027722\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 221, the loss is 4.865340899399535, parameters k is 13.57807906616618 and b is -62.63339284743623\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 222, the loss is 4.865172450319384, parameters k is 13.579278473280805 and b is -62.629440278266266\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 223, the loss is 4.865043757993635, parameters k is 13.578104757865786 and b is -62.6258829660133\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 224, the loss is 4.864912043900398, parameters k is 13.57930416498041 and b is -62.62193039684334\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 225, the loss is 4.864792760272218, parameters k is 13.57558578553377 and b is -62.61876834150737\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 226, the loss is 4.864622946958916, parameters k is 13.576785192648394 and b is -62.61481577233741\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 227, the loss is 4.864495618866325, parameters k is 13.575611477233375 and b is -62.61125846008444\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 228, the loss is 4.8643568992957285, parameters k is 13.576810884348 and b is -62.60730589091448\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 229, the loss is 4.864244621144908, parameters k is 13.573092504901359 and b is -62.60414383557851\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 230, the loss is 4.864074007340207, parameters k is 13.574291912015983 and b is -62.60019126640855\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 231, the loss is 4.863957882065731, parameters k is 13.575491319130608 and b is -62.59623869723859\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 232, the loss is 4.863823009618799, parameters k is 13.571772939683967 and b is -62.59307664190262\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 233, the loss is 4.863664261021396, parameters k is 13.572972346798592 and b is -62.58912407273266\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 234, the loss is 4.863525868212896, parameters k is 13.571798631383572 and b is -62.58556676047969\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 235, the loss is 4.8634027374610636, parameters k is 13.572998038498197 and b is -62.58161419130973\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 236, the loss is 4.863274870491483, parameters k is 13.569279659051556 and b is -62.57845213597376\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 237, the loss is 4.863114757660927, parameters k is 13.57047906616618 and b is -62.5744995668038\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 238, the loss is 4.8629777290855865, parameters k is 13.569305350751161 and b is -62.570942254550836\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 239, the loss is 4.862847909276324, parameters k is 13.570504757865786 and b is -62.566989685380875\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 240, the loss is 4.86270758849292, parameters k is 13.569331042450766 and b is -62.56343237312791\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 241, the loss is 4.862567267709516, parameters k is 13.568157327035747 and b is -62.559875060874944\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 242, the loss is 4.862427532477302, parameters k is 13.566983611620728 and b is -62.55631774862198\n",
      "k_gradient is -0.011994071146245747\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 243, the loss is 4.862300419324913, parameters k is 13.568183018735352 and b is -62.55236517945202\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 244, the loss is 4.862160098541507, parameters k is 13.567009303320333 and b is -62.54880786719905\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 245, the loss is 4.862019777758097, parameters k is 13.565835587905314 and b is -62.54525055494609\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 246, the loss is 4.8618794569746955, parameters k is 13.564661872490294 and b is -62.54169324269312\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 247, the loss is 4.861758218114394, parameters k is 13.563488157075275 and b is -62.538135930440156\n",
      "k_gradient is -0.04478853754940787\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 248, the loss is 4.86170337742771, parameters k is 13.567967010830216 and b is -62.5337881043532\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 249, the loss is 4.861480912063435, parameters k is 13.564248631383576 and b is -62.53062604901723\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 250, the loss is 4.861340591280022, parameters k is 13.563074915968556 and b is -62.527068736764264\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 251, the loss is 4.8612019193642855, parameters k is 13.561901200553537 and b is -62.5235114245113\n",
      "k_gradient is -0.021057312252965137\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 252, the loss is 4.861106189934548, parameters k is 13.564006931778833 and b is -62.51955885534134\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 253, the loss is 4.860956344147462, parameters k is 13.560288552332192 and b is -62.51639680000537\n",
      "k_gradient is -0.04478853754940787\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 254, the loss is 4.86089700064866, parameters k is 13.564767406087133 and b is -62.51204897391841\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 255, the loss is 4.860670029058704, parameters k is 13.561049026640493 and b is -62.50888691858244\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 256, the loss is 4.860529708275291, parameters k is 13.559875311225474 and b is -62.505329606329475\n",
      "k_gradient is 0.011737154150197016\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 257, the loss is 4.86041004027306, parameters k is 13.558701595810454 and b is -62.50177229407651\n",
      "k_gradient is -0.021057312252965137\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 258, the loss is 4.860299813155498, parameters k is 13.56080732703575 and b is -62.49781972490655\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 259, the loss is 4.860162787855431, parameters k is 13.55708894758911 and b is -62.49465766957058\n",
      "k_gradient is -0.021057312252965137\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 260, the loss is 4.86001488041166, parameters k is 13.559194678814405 and b is -62.49070510040062\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 261, the loss is 4.859917366835673, parameters k is 13.555476299367765 and b is -62.48754304506465\n",
      "k_gradient is -0.04478853754940787\n",
      "b_gradient is -0.043478260869565216\n",
      "Iteration 262, the loss is 4.859805691125781, parameters k is 13.559955153122706 and b is -62.48319521897769\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 263, the loss is 4.859602688635011, parameters k is 13.556236773676066 and b is -62.48003316364172\n",
      "k_gradient is -0.021057312252965137\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 264, the loss is 4.85952075838195, parameters k is 13.558342504901361 and b is -62.47608059447176\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 265, the loss is 4.859355436217377, parameters k is 13.55462412545472 and b is -62.47291853913579\n",
      "k_gradient is -0.021057312252965137\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 266, the loss is 4.8592358256381205, parameters k is 13.556729856680017 and b is -62.46896596996583\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 267, the loss is 4.859108183799758, parameters k is 13.553011477233376 and b is -62.46580391462986\n",
      "k_gradient is -0.021057312252965137\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 268, the loss is 4.858950892894289, parameters k is 13.555117208458672 and b is -62.4618513454599\n",
      "k_gradient is 0.03718379446640221\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 269, the loss is 4.8588609313821305, parameters k is 13.551398829012031 and b is -62.45868929012393\n",
      "k_gradient is -0.021057312252965137\n",
      "b_gradient is -0.03952569169960474\n",
      "Iteration 270, the loss is 4.858671135583913, parameters k is 13.553504560237327 and b is -62.45473672095397\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 271, the loss is 4.8585426642591765, parameters k is 13.553065627431003 and b is -62.451179408701\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 272, the loss is 4.858414192934434, parameters k is 13.55262669462468 and b is -62.447622096448036\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 273, the loss is 4.858285721609704, parameters k is 13.552187761818356 and b is -62.44406478419507\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 274, the loss is 4.858157250284968, parameters k is 13.551748829012032 and b is -62.440507471942105\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 275, the loss is 4.858028778960233, parameters k is 13.551309896205709 and b is -62.43695015968914\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 276, the loss is 4.857900307635501, parameters k is 13.550870963399385 and b is -62.433392847436174\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 277, the loss is 4.857771836310757, parameters k is 13.550432030593061 and b is -62.42983553518321\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 278, the loss is 4.857643364986025, parameters k is 13.549993097786738 and b is -62.42627822293024\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 279, the loss is 4.857514893661293, parameters k is 13.549554164980414 and b is -62.42272091067728\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 280, the loss is 4.857386422336554, parameters k is 13.54911523217409 and b is -62.41916359842431\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 281, the loss is 4.857257951011813, parameters k is 13.548676299367767 and b is -62.41560628617135\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 282, the loss is 4.857129479687081, parameters k is 13.548237366561443 and b is -62.41204897391838\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 283, the loss is 4.8570010083623485, parameters k is 13.54779843375512 and b is -62.408491661665416\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 284, the loss is 4.856872537037612, parameters k is 13.547359500948795 and b is -62.40493434941245\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 285, the loss is 4.856744065712876, parameters k is 13.546920568142472 and b is -62.401377037159484\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 286, the loss is 4.8566155943881375, parameters k is 13.546481635336148 and b is -62.39781972490652\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 287, the loss is 4.856487123063409, parameters k is 13.546042702529824 and b is -62.39426241265355\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 288, the loss is 4.856358651738669, parameters k is 13.5456037697235 and b is -62.39070510040059\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 289, the loss is 4.856230180413931, parameters k is 13.545164836917177 and b is -62.38714778814762\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 290, the loss is 4.856101709089201, parameters k is 13.544725904110853 and b is -62.38359047589466\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 291, the loss is 4.855973237764462, parameters k is 13.54428697130453 and b is -62.38003316364169\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 292, the loss is 4.8558447664397315, parameters k is 13.543848038498206 and b is -62.376475851388726\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 293, the loss is 4.855716295114988, parameters k is 13.543409105691882 and b is -62.37291853913576\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 294, the loss is 4.855587823790255, parameters k is 13.542970172885559 and b is -62.369361226882795\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 295, the loss is 4.855459352465515, parameters k is 13.542531240079235 and b is -62.36580391462983\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 296, the loss is 4.855330881140781, parameters k is 13.542092307272911 and b is -62.362246602376864\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 297, the loss is 4.855204817668533, parameters k is 13.541653374466588 and b is -62.3586892901239\n",
      "k_gradient is 0.02714426877470293\n",
      "b_gradient is -0.03162055335968379\n",
      "Iteration 298, the loss is 4.855078011124068, parameters k is 13.538938947589118 and b is -62.35552723478793\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n",
      "Iteration 299, the loss is 4.854949539799334, parameters k is 13.538500014782795 and b is -62.35196992253496\n",
      "k_gradient is 0.004389328063240359\n",
      "b_gradient is -0.035573122529644265\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "初始化参数\n",
    "\"\"\"\n",
    "\n",
    "k = random.random() * 200 - 100   # -100到100\n",
    "b = random.random() * 200 - 100\n",
    "\n",
    "learning_rate = 1e-1\n",
    "iteration_num = 300\n",
    "losses = []\n",
    "\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_pred = [price(r,k,b) for r in X_rm]\n",
    "    current_loss = loss(y, price_pred)\n",
    "    \n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_pred)\n",
    "    b_gradient = partial_derivative_b(y, price_pred)\n",
    "    print('k_gradient is {}'.format(k_gradient))\n",
    "    print('b_gradient is {}'.format(b_gradient))\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "    \n",
    "    \n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9c3d2f1cf8>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9c3d2d4cc0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
